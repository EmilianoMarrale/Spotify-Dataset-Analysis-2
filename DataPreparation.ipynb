{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import os\n",
    "from sktime.transformations.series.detrend import Detrender\n",
    "from sktime.transformations.series.detrend import Deseasonalizer\n",
    "from statsmodels.tsa.stattools import adfuller, kpss"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebd37c16f07a267",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artists_dataset = pd.read_csv('dataset/tabular/artists.csv')\n",
    "tracks_dataset = pd.read_csv('dataset/tabular/tracks.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Artists Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "526a367367eca526"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dropping rows with NaN and duplicated lines from artists dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ad3d709d5282ef6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Drop the rows with missing values\n",
    "artists_dataset = artists_dataset.dropna()\n",
    "# Drop the duplicated rows\n",
    "artists_dataset = artists_dataset.drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcc9019bc65f149b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Drop all the artists with same name and same genres\n",
    "artists_dataset = artists_dataset.sort_values('popularity', ascending=False).drop_duplicates(['name', 'genres'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2cacc6e06aad27f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tracks Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "146288a4369bdf28"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Aggregated all rows with duplicated 'id' values into a single row, keeping the unique genres in a list. Then dropped the duplicated rows from the original dataset and added the aggregated rows.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "191b0cfaeda7ee35"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "merged_df = tracks_dataset.groupby('id')['genre'].agg(list)\n",
    "df_merged = pd.merge(tracks_dataset, merged_df, on='id', how='left')\n",
    "\n",
    "# Find the indices of the rows with the highest popularity within each group (ID)\n",
    "indices_to_keep = df_merged.groupby('id')['popularity'].idxmax()\n",
    "# Filter the dataframe to keep only the rows with the highest popularity within each group\n",
    "tracks_dataset = df_merged.loc[indices_to_keep]\n",
    "tracks_dataset.rename(columns={'genre_y': 'genre'}, inplace=True)\n",
    "tracks_dataset = tracks_dataset.drop(columns=['genre_x'])\n",
    "# Display the resulting dataframe\n",
    "tracks_dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70c084b8e61512f0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e21777d3c5fee6fc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handling songs with duplicated names"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b64f6e5d787f10c5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Drop all the duplicates rows with same 'name' and 'artists' values, and keep only the row with the highest popularity\n",
    "tracks_dataset = tracks_dataset.sort_values('popularity', ascending=False).drop_duplicates(['name', 'artists'])\n",
    "# Display the resulting dataframe\n",
    "tracks_dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b45cb1e33ebd06",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating 3 new columns: 'release_year', 'release_month', 'release_day' from the 'release_date' column"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59e90586049e8ad2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Extract year, month, and day from 'album_release_date'\n",
    "# If 'album_release_date' is in YYYY-MM format, the day will be set as NaN\n",
    "\n",
    "# First, ensure 'album_release_date' is a string to safely apply string operations\n",
    "tracks_dataset['album_release_date'] = tracks_dataset['album_release_date'].astype(str)\n",
    "\n",
    "# Split 'album_release_date' into year, month, and day\n",
    "tracks_dataset['year'] = tracks_dataset['album_release_date'].apply(lambda x: x.split('-')[0])\n",
    "tracks_dataset['month'] = tracks_dataset['album_release_date'].apply(lambda x: x.split('-')[1] if len(x.split('-')) > 1 else 'NaN')\n",
    "tracks_dataset['day'] = tracks_dataset['album_release_date'].apply(lambda x: x.split('-')[2] if len(x.split('-')) > 2 else 'NaN')\n",
    "\n",
    "# Display the first few rows to verify the new columns\n",
    "tracks_dataset[['album_release_date', 'year', 'month', 'day']]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ded246a9f4194978",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dropping columns track_number, disc_number, album_type, album_total_tracks "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2fb9c7050214b9f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Drop the columns 'track_number', 'disc_number', 'album_type', 'album_total_tracks'\n",
    "tracks_dataset = tracks_dataset.drop(columns=['track_number', 'disc_number', 'album_type', 'album_total_tracks', 'album_release_date_precision', 'album_release_date'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e6d9de0ebc9f265",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Export the cleaned datasets to CSV files\n",
    "artists_dataset.to_csv('dataset/tabular/artists_cleaned.csv', index=False)\n",
    "tracks_dataset.to_csv('dataset/tabular/tracks_cleaned.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b1424d7a78cd2b0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c12de8b15446cf13",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Time Series Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c225c0199a8d683a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "LOADING THE DATASET"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f08f116040f1b1e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_npy(filename):\n",
    "    return np.load(filename)\n",
    "\n",
    "\n",
    "dir_path = 'dataset/time_series/'\n",
    "len_threshold = 1280\n",
    "X, y, ids = [], [], []\n",
    "\n",
    "for file in os.listdir(dir_path):\n",
    "    if os.path.splitext(file)[1] != '.npy':\n",
    "        continue\n",
    "\n",
    "    split = file.split(\"_\")\n",
    "    ids.append(split[0])  # track_id\n",
    "    y.append(split[1][:-4])  # genre\n",
    "    ts = load_npy(dir_path + file)\n",
    "\n",
    "    if len(ts) > len_threshold:\n",
    "        ts = ts[0:len_threshold]\n",
    "    else:\n",
    "        # pad = [np.mean(ts[:-5])] * (len_threshold-len(ts)) #Â fill by mean value of last n observations\n",
    "        pad = [ts[-1]] * (len_threshold - len(ts))  # fill with last observation\n",
    "        ts = np.append(ts, pad)\n",
    "\n",
    "    X.append([ts])\n",
    "\n",
    "X, y, ids = np.array(X), np.array(y), np.array(ids)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fb3b55cbeccc215",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performing offset translation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cfbeac6084deebe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    X[i] = X[i] - X[i].mean() #Offset translation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49f25cf8bcf0fe9a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performing amplitude scaling\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b435d05e16de19fc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    X[i] = (X[i] - X[i].mean()) / X[i].std() #Amplitue scaling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b725396542dbe38",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Noise removal"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5809aba2741fc9bf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "w = 100\n",
    "for i in range(len(X)):\n",
    "    ts = pd.Series(X[i,-1].T)\n",
    "    ts.rolling(window=w).mean() # Noise (smoothing)\n",
    "    X[i,-1] = ts.values\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1428d2097e82ac24",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check if Trend is present in the time series"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a6c9ef8a981b7fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Moving Average"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d28c46dca21a3f4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "N = 5\n",
    "slope_threshold = 0.0015\n",
    "trending_series_indices = []\n",
    "\n",
    "def estimate_slope(y_values, x_values):\n",
    "        slope = np.polyfit(x_values, y_values, 1)[0]\n",
    "        return slope\n",
    "\n",
    "def calculate_moving_average(data, window_size):\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "\n",
    "for i in range(len(X)):\n",
    "    moving_avg = calculate_moving_average(X[i,-1].T, N)\n",
    "    \n",
    "    x_values = np.arange(N-1, N-1 + len(moving_avg))\n",
    "    \n",
    "    slope = estimate_slope(moving_avg, x_values)\n",
    "    if slope > slope_threshold:\n",
    "        trending_series_indices.append(i)\n",
    "\n",
    "print(f\"Indices of trending time series: {trending_series_indices}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "361b0283d3cfa571",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(len(trending_series_indices))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd11952c82ba76d5",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Here we have the time series with a trand that needs detrending"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10aa4395644c3233"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Apply detrending to the trending time series\n",
    "\n",
    "detrenders = Detrender()\n",
    "\n",
    "for i in trending_series_indices:\n",
    "    ts = pd.Series(X[i,-1].T)\n",
    "    plt.plot(ts)\n",
    "    ts_detrended = detrenders.fit_transform(ts)\n",
    "    X[i,-1] = ts_detrended.values\n",
    "    plt.plot(ts_detrended)\n",
    "    plt.legend(['Original', 'Detrended'])\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9f20d2d5487da2e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exporting the cleaned time series data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df83b57dc6eca8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Directory di output per i file npy\n",
    "output_directory = \"./dataset/cleaned_time_series\"\n",
    "\n",
    "# Itera attraverso ciascuna serie temporale e salvala in un file npy\n",
    "for i, serie in enumerate(X):\n",
    "    # Trasponi la serie temporale prima di salvarla\n",
    "    serie_trasposta = serie.reshape(-1, 1)  # Rappresenta la serie temporale come una singola colonna\n",
    "    filename = f\"{ids[i]}_{y[i]}.npy\"\n",
    "    np.save(os.path.join(output_directory, filename), serie_trasposta)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ed9516ff058b396",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating a dataset with features extracted from the time series data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b15dde5a6d94b95"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create a dataframe with features extracted from the time series data\n",
    "features = pd.DataFrame()\n",
    "\n",
    "# Add the mean value of each time series as a feature\n",
    "for i in range(len(X)):\n",
    "    features.loc[i, 'mean'] = np.mean(X[i,-1].T)\n",
    "    features.loc[i, 'std'] = np.std(X[i,-1].T)\n",
    "    features.loc[i, 'min'] = np.min(X[i,-1].T)\n",
    "    features.loc[i, 'max'] = np.max(X[i,-1].T)\n",
    "    features.loc[i, 'median'] = np.median(X[i,-1].T)\n",
    "    features.loc[i, '25th_percentile'] = np.percentile(X[i,-1].T, 25)\n",
    "    features.loc[i, '75th_percentile'] = np.percentile(X[i,-1].T, 75)\n",
    "    features.loc[i, 'interquartile_range'] = np.percentile(X[i,-1].T, 75) - np.percentile(X[i,-1].T, 25)\n",
    "    features.loc[i, 'skewness'] = pd.Series(X[i,-1].T).skew()\n",
    "    features.loc[i, 'kurtosis'] = pd.Series(X[i,-1].T).kurtosis()\n",
    "    features.loc[i, 'autocorrelation'] = pd.Series(X[i,-1].T).autocorr()\n",
    "    features.loc[i, 'trend'] = estimate_slope(X[i,-1].T, np.arange(len(X[i,-1].T)))\n",
    "    features.loc[i, 'ids'] = ids[i]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82d96bdf43557054",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
